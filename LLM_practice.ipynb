{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM configuration practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM configuratino basic process\n",
    "\n",
    "1. pretraining(사전 훈련)으로 일반적인 언어 능력을 가르친다\n",
    "\n",
    "2. fine tuning(미세조정) 단계에서 특정 용도에 맞게 다듬는다   \n",
    "\n",
    "    * 여기까지가 기본적인 절차\n",
    "\n",
    "3. 데이터베이스(혹은 인터넷) 검색 기능을 추가한다   \n",
    "\n",
    "    * 지식의 범위와 정확성을 더 높일 수 있다\n",
    "    \n",
    "4. 내부적으로 질의를 반복하여 더 좋은 결론을 도출한다   \n",
    "\n",
    "    * 사람이 생각을 거듭하여 더 깊이 있는 결론을 이끌어내는 과정과 같다\n",
    "\n",
    "#### the stage of practice   \n",
    "\n",
    "1. 훈련 데이터 준비\n",
    "\n",
    "2. 데이터 로더 정의\n",
    "\n",
    "3. 모델 정의\n",
    "\n",
    "4. 훈련\n",
    "\n",
    "5. 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computer\n",
    "\n",
    "* macbook pro m4 pro\n",
    "\n",
    "#### packages\n",
    "\n",
    "* miniconda=24.11.1\n",
    "\n",
    "* python=3.9.20\n",
    "    ```sh\n",
    "    conda create -n LLM python=3.9.20\n",
    "    ```\n",
    "\n",
    "* tiktoken=0.9.0\n",
    "    ```sh\n",
    "    pip install tiktoken\n",
    "    ```\n",
    "\n",
    "* torch=2.6.0\n",
    "    ```sh\n",
    "    pip3 install torch torchvision torchaudio\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 훈련 데이터 준비\n",
    "\n",
    "* [해리포터 영어 원서](https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books?select=02+Harry+Potter+and+the+Chamber+of+Secrets.txt)\n",
    "    ```sh\n",
    "    curl -o LLM_configuration/data/file-name.txt \"download link\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(filename, path):\n",
    "    with open(path + '/' + filename,  'r', encoding='utf-8') as file:\n",
    "        book_text = file.read()\n",
    "        \n",
    "    cleaned_text = re.sub(r'\\n+', ' ', book_text) # 줄 바꿈을 공백으로\n",
    "    cleaned_text = re.sub(r's+', ' ', cleaned_text) # 연속되는 공백을 하나의 공백으로\n",
    "    \n",
    "    print('cleaned_' + filename, len(cleaned_text), 'characters') # 글자 수 출력\n",
    "    \n",
    "    with open(path + '/' + 'cleaned_' + filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text) # 잘 정리된 훈련 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_01_Harry_Potter_and_the_Sorcerers_Stone.txt 435335 characters\n",
      "cleaned_02_Harry_Potter_and_the_Chamber_of_Secrets.txt 487913 characters\n",
      "cleaned_03_Harry_Potter_and_the_Prisoner_of_Azkaban.txt 619717 characters\n",
      "cleaned_04_Harry_Potter_and_the_Goblet_of_Fire.txt 1091860 characters\n",
      "cleaned_05_Harry_Potter_and_the_Order_of_the_Phoenix.txt 1486941 characters\n",
      "cleaned_06_Harry_Potter_and_the_Half-Blood_Prince.txt 980175 characters\n",
      "cleaned_07_Harry_Potter_and_the_Deathly_Hallows.txt 1131296 characters\n"
     ]
    }
   ],
   "source": [
    "filename_list = ['01_Harry_Potter_and_the_Sorcerers_Stone.txt',\n",
    "                 '02_Harry_Potter_and_the_Chamber_of_Secrets.txt',\n",
    "                 '03_Harry_Potter_and_the_Prisoner_of_Azkaban.txt',\n",
    "                 '04_Harry_Potter_and_the_Goblet_of_Fire.txt',\n",
    "                 '05_Harry_Potter_and_the_Order_of_the_Phoenix.txt',\n",
    "                 '06_Harry_Potter_and_the_Half-Blood_Prince.txt',\n",
    "                 '07_Harry_Potter_and_the_Deathly_Hallows.txt']\n",
    "file_path = '/Users/kh22cho/LLM_configuration/data'\n",
    "\n",
    "for filename in filename_list:\n",
    "    clean_text(filename, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화 진행\n",
    "\n",
    "UTF-8 BPE (Bype Pair Encoding)\n",
    "\n",
    "문자열 데이터를 숫자로 변환하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2') # gpt2에서 사용하는 tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화의 간단한 예시\n",
    "\n",
    "```python\n",
    "text = 'Harry Potter was a wizard.' # token으로 바꿀 예시 문장 | neural net이 이해할 수 있게 적당히 자른 것이 token\n",
    "\n",
    "tokens = tokenizer.encode(text) # text를 숫자로 바꾸는 과정 \n",
    "\n",
    "print('글자수 :', len(text), '토큰수 :', len(tokens))\n",
    "print(tokens) # 6개의 token들로 text를 변환함 | .도 문장의 끝이라는 의미의 token\n",
    "print(tokenizer.decode(tokens)) # 6개의 token들을 다시 decode하여 text로 원상 복구\n",
    "for t in tokens:\n",
    "    print(f'{t} >>> {tokenizer.decode([t])}')\n",
    "```\n",
    "```\n",
    "글자수 : 26 토큰수 : 6   \n",
    "[18308, 14179, 373, 257, 18731, 13]   \n",
    "Harry Potter was a wizard.   \n",
    "18308 >>> Harry   \n",
    "14179 >>>  Potter   \n",
    "373 >>>  was   \n",
    "257 >>>  a   \n",
    "18731 >>>  wizard   \n",
    "13 >>> .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한글과 한자가 섞여있는 문장에 대한 tokenizer 예제\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer # pip install transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\")  # KoGPT2 사용 | 가장 잘되었던 모델\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")  # KoGPT2 사용\n",
    "\n",
    "print(\"Vocab size :\", len(tokenizer))\n",
    "\n",
    "text = \"대사께서는 도(道)를 얻은 모양이구려.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(len(text), len(tokens))\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 글자 단위로 토큰화하는 예시 | 비효율적이므로 잘 사용하지 않음\n",
    "\n",
    "```python\n",
    "for char in text:\n",
    "    token_ids = tokenizer.encode(char) # 한 글자씩 인코딩(토큰화)\n",
    "    decoded = tokenizer.decode(token_ids) # 한 글자씩 디코딩\n",
    "    print(f\"{char} >>> {token_ids} >>> {decoded}\") # 한 글자 단위이므로 공백도 토큰이 됨\n",
    "```\n",
    "```\n",
    "H >>> [39] >>> H\n",
    "a >>> [64] >>> a\n",
    "r >>> [81] >>> r\n",
    "r >>> [81] >>> r\n",
    "y >>> [88] >>> y\n",
    "  >>> [220] >>>  \n",
    "P >>> [47] >>> P\n",
    "o >>> [78] >>> o\n",
    "t >>> [83] >>> t\n",
    "t >>> [83] >>> t\n",
    "e >>> [68] >>> e\n",
    "r >>> [81] >>> r\n",
    "  >>> [220] >>>  \n",
    "w >>> [86] >>> w\n",
    "a >>> [64] >>> a\n",
    "s >>> [82] >>> s\n",
    "  >>> [220] >>>  \n",
    "a >>> [64] >>> a\n",
    "  >>> [220] >>>  \n",
    "w >>> [86] >>> w\n",
    "i >>> [72] >>> i\n",
    "z >>> [89] >>> z\n",
    "a >>> [64] >>> a\n",
    "r >>> [81] >>> r\n",
    "d >>> [67] >>> d\n",
    ". >>> [13] >>> .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로더 (DataLoader)\n",
    "\n",
    "입력 데이터가 많아질수록 neural net도 무시무시하게 커지기 때문에\n",
    "\n",
    "전체 데이터를 한번에 사용하지 않고, 몇개씩 쪼개주는 기능을 하는 것이 ```Dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt, max_length = 32, stride = 4):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # token_ids = tokenizer.encode(\"<|endoftext|>\" + txt, allowed_special={\"<|endoftext|>\"})\n",
    "        token_ids = tokenizer.encode(txt) # 책 한권을 token으로 바꿈\n",
    "        \n",
    "        print('# of tokens in txt :', len(token_ids)) # token이 몇개정도 되는지\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1] # 앞의 몇개의 단어(input)가 오면, 그 다음으로 오는 단어(target)를 의미\n",
    "            self.input_ids.append(torch.tensor(input_chunk)) # input를 넣으면, \n",
    "            self.target_ids.append(torch.tensor(target_chunk)) # target하고 가급적 똑같은 대답을 하도록 훈련을 시킴\n",
    "            # example\n",
    "            # Text :   Harry Potter was a Wizard.\n",
    "            # input :  Harry Poter\n",
    "            # target :              was\n",
    "            \n",
    "    def __len__(self): # 훈련을 시킬 수 있는 조각이 전부 몇개인지 알려주는 함수\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx): # DataLoader를 사용할때 실제로 호출하는 함수\n",
    "        return self.input_ids[idx], self.target_ids[idx] # input의 id와 target의 id를 쌍으로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in txt : 138857\n"
     ]
    }
   ],
   "source": [
    "# with open('cleaned_한글문서.txt', 'r', encoding='utf-8-sig') as file: # 선택 : -sig를 붙여서 BOM 제거\n",
    "with open(file_path + '/' + 'cleaned_01_Harry_Potter_and_the_Sorcerers_Stone.txt', 'r', encoding='utf-8-sig') as file: # 선택 : -sig를 붙여서 BOM 제거\n",
    "    txt = file.read() # 책을 읽어온다\n",
    "\n",
    "dataset = MyDataset(txt, max_length = 32, stride = 4) # dataset을 정의\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True) # dataset을 이용하여 pytorch의 DataLoader를 만든다\n",
    "\n",
    "# 주의 : 여기서는 코드를 단순화하기 위해 test, valid는 생략하고 train_loader만 만들었습니다\n",
    "#       관련된 ML 이론은 train vs test vs validation 등으로 검색하여 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader가 내부적으로 어떻게 작동하는지\n",
    "```python\n",
    "dataiter = iter(train_loader) # loader를 iterator로 바꿔주고\n",
    "\n",
    "x, y = next(dataiter) # next를 이용하여 실제 x, y로 받아온다 | x, y는 각각 input, target이며 token이다\n",
    "\n",
    "print(tokenizer.decode(x[0].tolist()))\n",
    "print(tokenizer.decode(y[0].tolist())) # target이 하나 밀린 형태\n",
    "```\n",
    "```\n",
    "an fer to Slytherin at once, becau e it wa  hi  de tiny. Harry told the turban he didn’t want to\n",
    " fer to Slytherin at once, becau e it wa  hi  de tiny. Harry told the turban he didn’t want to be\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print(x[0])\n",
    "print(y[0]) # target이 하나 밀린 형태 | token으로 보면 더 명확함\n",
    "```\n",
    "```\n",
    "tensor([  272, 11354,   284, 31615,   490,   259,   379,  1752,    11,   639,\n",
    "          559,   304,   340,  2082,   220, 23105,   220,   390,  7009,    13,\n",
    "         5850,  1297,   262,  7858,  3820,   339,  1422,   447,   247,    83,\n",
    "          765,   284])\n",
    "tensor([11354,   284, 31615,   490,   259,   379,  1752,    11,   639,   559,\n",
    "          304,   340,  2082,   220, 23105,   220,   390,  7009,    13,  5850,\n",
    "         1297,   262,  7858,  3820,   339,  1422,   447,   247,    83,   765,\n",
    "          284,   307])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 정의\n",
    "\n",
    "뉴럴 네트워크 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
