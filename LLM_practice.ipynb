{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM configuration practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM configuratino basic process\n",
    "\n",
    "1. pretraining(사전 훈련)으로 일반적인 언어 능력을 가르친다\n",
    "\n",
    "2. fine tuning(미세조정) 단계에서 특정 용도에 맞게 다듬는다   \n",
    "\n",
    "    > 여기까지가 기본적인 절차   \n",
    "\n",
    "\n",
    "3. 데이터베이스(혹은 인터넷) 검색 기능을 추가한다   \n",
    "\n",
    "    > 지식의 범위와 정확성을 더 높일 수 있다   \n",
    "    \n",
    "\n",
    "4. 내부적으로 질의를 반복하여 더 좋은 결론을 도출한다   \n",
    "\n",
    "    > 사람이 생각을 거듭하여 더 깊이 있는 결론을 이끌어내는 과정과 같다   \n",
    "\n",
    "\n",
    "#### the stage of practice   \n",
    "\n",
    "1. 훈련 데이터 준비\n",
    "\n",
    "2. 데이터 로더 정의\n",
    "\n",
    "3. 모델 정의\n",
    "\n",
    "4. 훈련\n",
    "\n",
    "5. 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computer\n",
    "\n",
    "* macbook pro m4 pro\n",
    "\n",
    "#### packages\n",
    "\n",
    "* miniconda=24.11.1\n",
    "\n",
    "* python=3.9.20\n",
    "    ```sh\n",
    "    conda create -n LLM python=3.9.20\n",
    "    ```\n",
    "\n",
    "* tiktoken=0.9.0\n",
    "    ```sh\n",
    "    pip install tiktoken\n",
    "    ```\n",
    "\n",
    "* torch=2.6.0\n",
    "    ```sh\n",
    "    pip3 install torch torchvision torchaudio\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 훈련 데이터 준비\n",
    "\n",
    "* [해리포터 영어 원서](https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books?select=02+Harry+Potter+and+the+Chamber+of+Secrets.txt)\n",
    "    ```sh\n",
    "    curl -o LLM_configuration/data/file-name.txt \"download link\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(filename, path):\n",
    "    with open(path + '/' + filename,  'r', encoding='utf-8') as file:\n",
    "        book_text = file.read()\n",
    "        \n",
    "    cleaned_text = re.sub(r'\\n+', ' ', book_text) # 줄 바꿈을 공백으로\n",
    "    cleaned_text = re.sub(r's+', ' ', cleaned_text) # 연속되는 공백을 하나의 공백으로\n",
    "    \n",
    "    print('cleaned_' + filename, len(cleaned_text), 'characters') # 글자 수 출력\n",
    "    \n",
    "    with open(path + '/' + 'cleaned_' + filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text) # 잘 정리된 훈련 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_01_Harry_Potter_and_the_Sorcerers_Stone.txt 435335 characters\n",
      "cleaned_02_Harry_Potter_and_the_Chamber_of_Secrets.txt 487913 characters\n",
      "cleaned_03_Harry_Potter_and_the_Prisoner_of_Azkaban.txt 619717 characters\n",
      "cleaned_04_Harry_Potter_and_the_Goblet_of_Fire.txt 1091860 characters\n",
      "cleaned_05_Harry_Potter_and_the_Order_of_the_Phoenix.txt 1486941 characters\n",
      "cleaned_06_Harry_Potter_and_the_Half-Blood_Prince.txt 980175 characters\n",
      "cleaned_07_Harry_Potter_and_the_Deathly_Hallows.txt 1131296 characters\n"
     ]
    }
   ],
   "source": [
    "filename_list = ['01_Harry_Potter_and_the_Sorcerers_Stone.txt',\n",
    "                 '02_Harry_Potter_and_the_Chamber_of_Secrets.txt',\n",
    "                 '03_Harry_Potter_and_the_Prisoner_of_Azkaban.txt',\n",
    "                 '04_Harry_Potter_and_the_Goblet_of_Fire.txt',\n",
    "                 '05_Harry_Potter_and_the_Order_of_the_Phoenix.txt',\n",
    "                 '06_Harry_Potter_and_the_Half-Blood_Prince.txt',\n",
    "                 '07_Harry_Potter_and_the_Deathly_Hallows.txt']\n",
    "file_path = '/Users/kh22cho/LLM_configuration/data'\n",
    "\n",
    "for filename in filename_list:\n",
    "    clean_text(filename, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화 진행\n",
    "\n",
    "UTF-8 BPE (Bype Pair Encoding)\n",
    "\n",
    "문자열 데이터를 숫자로 변환하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2') # gpt2에서 사용하는 tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화의 간단한 예시\n",
    "\n",
    "```python\n",
    "text = 'Harry Potter was a wizard.' # token으로 바꿀 예시 문장 | neural net이 이해할 수 있게 적당히 자른 것이 token\n",
    "\n",
    "tokens = tokenizer.encode(text) # text를 숫자로 바꾸는 과정 \n",
    "\n",
    "print('글자수 :', len(text), '토큰수 :', len(tokens))\n",
    "print(tokens) # 6개의 token들로 text를 변환함 | .도 문장의 끝이라는 의미의 token\n",
    "print(tokenizer.decode(tokens)) # 6개의 token들을 다시 decode하여 text로 원상 복구\n",
    "for t in tokens:\n",
    "    print(f'{t} >>> {tokenizer.decode([t])}')\n",
    "```\n",
    "```\n",
    "글자수 : 26 토큰수 : 6   \n",
    "[18308, 14179, 373, 257, 18731, 13]   \n",
    "Harry Potter was a wizard.   \n",
    "18308 >>> Harry   \n",
    "14179 >>>  Potter   \n",
    "373 >>>  was   \n",
    "257 >>>  a   \n",
    "18731 >>>  wizard   \n",
    "13 >>> .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한글과 한자가 섞여있는 문장에 대한 tokenizer 예제\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer # pip install transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\")  # KoGPT2 사용 | 가장 잘되었던 모델\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")  # KoGPT2 사용\n",
    "\n",
    "print(\"Vocab size :\", len(tokenizer))\n",
    "\n",
    "text = \"대사께서는 도(道)를 얻은 모양이구려.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(len(text), len(tokens))\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 글자 단위로 토큰화하는 예시 | 비효율적이므로 잘 사용하지 않음\n",
    "\n",
    "```python\n",
    "for char in text:\n",
    "    token_ids = tokenizer.encode(char) # 한 글자씩 인코딩(토큰화)\n",
    "    decoded = tokenizer.decode(token_ids) # 한 글자씩 디코딩\n",
    "    print(f\"{char} >>> {token_ids} >>> {decoded}\") # 한 글자 단위이므로 공백도 토큰이 됨\n",
    "```\n",
    "```\n",
    "H >>> [39] >>> H\n",
    "a >>> [64] >>> a\n",
    "r >>> [81] >>> r\n",
    "r >>> [81] >>> r\n",
    "y >>> [88] >>> y\n",
    "  >>> [220] >>>  \n",
    "P >>> [47] >>> P\n",
    "o >>> [78] >>> o\n",
    "t >>> [83] >>> t\n",
    "t >>> [83] >>> t\n",
    "e >>> [68] >>> e\n",
    "r >>> [81] >>> r\n",
    "  >>> [220] >>>  \n",
    "w >>> [86] >>> w\n",
    "a >>> [64] >>> a\n",
    "s >>> [82] >>> s\n",
    "  >>> [220] >>>  \n",
    "a >>> [64] >>> a\n",
    "  >>> [220] >>>  \n",
    "w >>> [86] >>> w\n",
    "i >>> [72] >>> i\n",
    "z >>> [89] >>> z\n",
    "a >>> [64] >>> a\n",
    "r >>> [81] >>> r\n",
    "d >>> [67] >>> d\n",
    ". >>> [13] >>> .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로더 (DataLoader)\n",
    "\n",
    "입력 데이터가 많아질수록 neural net도 무시무시하게 커지기 때문에\n",
    "\n",
    "전체 데이터를 한번에 사용하지 않고, 몇개씩 쪼개주는 기능을 하는 것이 ```Dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt, max_length = 32, stride = 4):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # token_ids = tokenizer.encode(\"<|endoftext|>\" + txt, allowed_special={\"<|endoftext|>\"})\n",
    "        token_ids = tokenizer.encode(txt) # 책 한권을 token으로 바꿈\n",
    "        \n",
    "        print('# of tokens in txt :', len(token_ids)) # token이 몇개정도 되는지\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1] # 앞의 몇개의 단어(input)가 오면, 그 다음으로 오는 단어(target)를 의미\n",
    "            self.input_ids.append(torch.tensor(input_chunk)) # input를 넣으면, \n",
    "            self.target_ids.append(torch.tensor(target_chunk)) # target하고 가급적 똑같은 대답을 하도록 훈련을 시킴\n",
    "            # example\n",
    "            # Text :   Harry Potter was a Wizard.\n",
    "            # input :  Harry Poter\n",
    "            # target :              was\n",
    "            \n",
    "    def __len__(self): # 훈련을 시킬 수 있는 조각이 전부 몇개인지 알려주는 함수\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx): # DataLoader를 사용할때 실제로 호출하는 함수\n",
    "        return self.input_ids[idx], self.target_ids[idx] # input의 id와 target의 id를 쌍으로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harry Potter 모든 책을 합쳐서 이용할 시\n",
    "\n",
    "```python\n",
    "txt = \"\" # 여러 책의 내용을 합칠 문자열\n",
    "for f in filename_list:\n",
    "    with open(file_path + '/' + 'cleaned_' + f, 'r', encoding='utf-8-sig') as file: # 선택 : -sig를 붙여서 BOM 제거\n",
    "        print('open', f)\n",
    "        txt += file.read() # 책을 읽어온다\n",
    "\n",
    "# gpu 사용이 어려울 때, txt의 일부만 이용하여 cpu에 부담을 줄이고 싶을때\n",
    "# txt = txt[:100000]\n",
    "```\n",
    "\n",
    "```\n",
    "open 01_Harry_Potter_and_the_Sorcerers_Stone.txt\n",
    "open 02_Harry_Potter_and_the_Chamber_of_Secrets.txt\n",
    "open 03_Harry_Potter_and_the_Prisoner_of_Azkaban.txt\n",
    "open 04_Harry_Potter_and_the_Goblet_of_Fire.txt\n",
    "open 05_Harry_Potter_and_the_Order_of_the_Phoenix.txt\n",
    "open 06_Harry_Potter_and_the_Half-Blood_Prince.txt\n",
    "open 07_Harry_Potter_and_the_Deathly_Hallows.txt\n",
    "# of tokens in txt : 1912071\n",
    "```\n",
    "\n",
    "mps는 너무 오래걸려서 책 한권만 이용하기로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in txt : 138857\n"
     ]
    }
   ],
   "source": [
    "# with open('cleaned_한글문서.txt', 'r', encoding='utf-8-sig') as file: # 선택 : -sig를 붙여서 BOM 제거\n",
    "with open(file_path + '/' + 'cleaned_01_Harry_Potter_and_the_Sorcerers_Stone.txt', 'r', encoding='utf-8-sig') as file: # 선택: -sig를 붙여서 BOM 제거\n",
    "    txt = file.read()\n",
    "\n",
    "dataset = MyDataset(txt, max_length = 32, stride = 4) # dataset을 정의\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True) # dataset을 이용하여 pytorch의 DataLoader를 만든다\n",
    "\n",
    "# 주의 : 여기서는 코드를 단순화하기 위해 test, valid는 생략하고 train_loader만 만들었습니다\n",
    "#       관련된 ML 이론은 train vs test vs validation 등으로 검색하여 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader가 내부적으로 어떻게 작동하는지\n",
    "```python\n",
    "dataiter = iter(train_loader) # loader를 iterator로 바꿔주고\n",
    "\n",
    "x, y = next(dataiter) # next를 이용하여 실제 x, y로 받아온다 | x, y는 각각 input, target이며 token이다\n",
    "\n",
    "print(tokenizer.decode(x[0].tolist()))\n",
    "print(tokenizer.decode(y[0].tolist())) # target이 하나 밀린 형태\n",
    "```\n",
    "```\n",
    "an fer to Slytherin at once, becau e it wa  hi  de tiny. Harry told the turban he didn’t want to\n",
    " fer to Slytherin at once, becau e it wa  hi  de tiny. Harry told the turban he didn’t want to be\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print(x[0])\n",
    "print(y[0]) # target이 하나 밀린 형태 | token으로 보면 더 명확함\n",
    "```\n",
    "```\n",
    "tensor([  272, 11354,   284, 31615,   490,   259,   379,  1752,    11,   639,\n",
    "          559,   304,   340,  2082,   220, 23105,   220,   390,  7009,    13,\n",
    "         5850,  1297,   262,  7858,  3820,   339,  1422,   447,   247,    83,\n",
    "          765,   284])\n",
    "tensor([11354,   284, 31615,   490,   259,   379,  1752,    11,   639,   559,\n",
    "          304,   340,  2082,   220, 23105,   220,   390,  7009,    13,  5850,\n",
    "         1297,   262,  7858,  3820,   339,  1422,   447,   247,    83,   765,\n",
    "          284,   307])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 정의\n",
    "\n",
    "뉴럴 네트워크 모델 정의\n",
    "\n",
    "모델은 교재([Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch))에서 제공하는 예제 코드를 약간 수정한 정도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's parameter\n",
    "\n",
    "# 50257 Tiktoken | 사전에 단어가 몇개나 있는지\n",
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "# VOCAB_SIZE = len(tokenizer) # AutoTokenizer일 경우\n",
    "\n",
    "CONTEXT_LENGTH = 128 # Shortened context length (orig : 1024)\n",
    "EMB_DIM = 768 # Embedding dimention\n",
    "NUM_HEADS = 12 # Number of attention heads\n",
    "NUM_LAYERS = 12 # Number of layers\n",
    "DROP_RATE = 0.1 # Dropout rate\n",
    "QKV_BIAS = False # Query-key-value bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module): # self attention의 핵심이 되는 부분\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_out % NUM_HEADS == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // NUM_HEADS\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(DROP_RATE)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        values = values.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module): # neural net 가중치 값들이 너무 들쑥날쑥하지 않고 적당한 범위 내에 몰려있도록 만들어줌\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module): # Linear, GELU activation 다음에 다시 Linear가 있는 간단한 구조\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(EMB_DIM, 4 * EMB_DIM),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * EMB_DIM, EMB_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module): # MultiHeadAttention, FeedForward, LayerNorm등이 사용됨\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=EMB_DIM,\n",
    "            d_out=EMB_DIM)\n",
    "    \n",
    "        self.ff = FeedForward()\n",
    "        self.norm1 = LayerNorm(EMB_DIM)\n",
    "        self.norm2 = LayerNorm(EMB_DIM)\n",
    "        self.drop_shortcut = nn.Dropout(DROP_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module): # GPT model을 사용\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, EMB_DIM)\n",
    "        self.drop_emb = nn.Dropout(DROP_RATE)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock() for _ in range(NUM_LAYERS)]) # TransformerBlock이 정의된 부분으로 이동하면,\n",
    "\n",
    "        self.final_norm = LayerNorm(EMB_DIM)\n",
    "        self.out_head = nn.Linear(EMB_DIM, VOCAB_SIZE, bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer architecture\n",
    "\n",
    "Transformer 구조 중에서 Masked Multi-Head Attention이라는 것은 훈련시키는 기본 원리와 밀접한 관련이 있음\n",
    "\n",
    "<img src=\"figure/Transformer_architecture.png\" width=\"30%\">\n",
    "\n",
    "화살표 왼쪽 그림의 행렬을 보면, 단어끼리 얼마나 관계가 깊은지에 대한 값을 저장한 행렬\n",
    "\n",
    "이번에는 화살표 오른쪽 그림의 오른쪽 윗부분을 보면, 숨겨져있는 부분이 있는데 이것이 Masked Multi-Head Attention\n",
    "\n",
    "데이터 셋을 만들때 다음 단어를 추측하는 것과 연관이 되어있음\n",
    "\n",
    "앞부분과 뒷부분을 같이 주면, 다음단어라는 정답을 주는 꼴이 되어버리므로 숨겨서 학습\n",
    "\n",
    "즉, 미래에 오는 단어를 모르게 학습을 하도록 네트워크 구조를 만든 것\n",
    "\n",
    "<img src=\"figure/Masked_Multi_Head_Attention.webp\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GELU activation function\n",
    "\n",
    "<img src=\"./figure/GELU.png\" width=\"40%\" height=\"30%\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 훈련\n",
    "\n",
    "nvidia GPU가 아닌 silicon mac의 GPU를 사용하려면 조금 다르게 셋팅이 필요\n",
    "\n",
    "apple에서는 GPU 대신 MPS(Metal Performance Shaders)라고 함\n",
    "\n",
    "MPS는 기존의 CUDA 또는 OpenCL과 같은 라이브러리와 달리 애플에서 직접 제공하는 프레임 워크, 따라서 애플 GPU 기술에 최적화되어 있음\n",
    "\n",
    "- [How to use GPU Acceleration in silicon mac (MPS setting)](https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available in current env : True\n",
      "PyTorch is built with MPS activation : True\n",
      "Current device : mps\n"
     ]
    }
   ],
   "source": [
    "# PyTorch에서 MPS가 활성화되어 있는지 확인하는 방법\n",
    "import torch\n",
    "\n",
    "# 현재 환경에서 MPS를 사용할 수 있는지 여부\n",
    "print('GPU available in current env :', torch.backends.mps.is_available())\n",
    "\n",
    "# 현재 PyTorch 설치가 MPS 활성화로 빌드되었는지 여부 \n",
    "# True : 현재 설치된 PyTorch가 MPS 활성화로 빌드 되었음\n",
    "print('PyTorch is built with MPS activation :', torch.backends.mps.is_built())\n",
    "\n",
    "# MPS를 사용하기 위한 디바이스를 설정\n",
    "device = torch.device('mps')\n",
    "print(\"Current device :\", device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = GPTModel()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens seen : 1024\n",
      "Tokens seen : 1025024\n",
      "Epoch : 1, Loss : 3.357061286144151\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 2049024\n",
      "Epoch : 2, Loss : 1.2335034265729334\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 3073024\n",
      "Epoch : 3, Loss : 0.49334410270091794\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 4097024\n",
      "Epoch : 4, Loss : 0.37944563458986386\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 5121024\n",
      "Epoch : 5, Loss : 0.3425737563864331\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 6145024\n",
      "Epoch : 6, Loss : 0.32018947733284364\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 7169024\n",
      "Epoch : 7, Loss : 0.30283895357618473\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 8193024\n",
      "Epoch : 8, Loss : 0.2903618865072507\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 9217024\n",
      "Epoch : 9, Loss : 0.2819105089398108\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 10241024\n",
      "Epoch : 10, Loss : 0.27422030606887876\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 11265024\n",
      "Epoch : 11, Loss : 0.266469235074916\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 12289024\n",
      "Tokens seen : 13313024\n",
      "Epoch : 12, Loss : 0.2621880896884357\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 14337024\n",
      "Epoch : 13, Loss : 0.25707418444081864\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 15361024\n",
      "Epoch : 14, Loss : 0.25297127213205356\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 16385024\n",
      "Epoch : 15, Loss : 0.25029579835954185\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 17409024\n",
      "Epoch : 16, Loss : 0.2458168430961806\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 18433024\n",
      "Epoch : 17, Loss : 0.24469778641398543\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 19457024\n",
      "Epoch : 18, Loss : 0.2408429860958754\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 20481024\n",
      "Epoch : 19, Loss : 0.23852268415348557\n",
      "Allocated GPU Memory: 2.89 GB\n",
      "Tokens seen : 21505024\n",
      "Epoch : 20, Loss : 0.23795315909209727\n",
      "Allocated GPU Memory: 2.89 GB\n"
     ]
    }
   ],
   "source": [
    "tokens_seen, global_step = 0, -1\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train() # Set model to training mode\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # input하고 target을 받아 neural net에 넣음 \n",
    "    # 이때 input과 target을 하나씩이 아닌 묶음, batch로 넣음\n",
    "    # 만약 gpu 메모리가 적은 경우, train_loader 초기화 과정에서 batch size를 낮추면 됨\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "        \n",
    "        # train_loader로부터 input, target batch를 받아 device로 복사\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "        \n",
    "        logits = model(input_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # 대답을 얼마나 잘했는지 평가\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward() # Calculate loss gradients\n",
    "        optimizer.step() # Update model weights using loss gradients | 대답을 더 잘하도록 가중치 업데이트\n",
    "        tokens_seen += input_batch.numel()\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step %1000 == 0:\n",
    "            print(f'Tokens seen : {tokens_seen}')\n",
    "        # Optional evaluation step\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f'Epoch : {epoch + 1}, Loss : {avg_loss}')\n",
    "    # 에포크가 끝난 후 메모리 사용량 출력\n",
    "    print(f\"Allocated GPU Memory: {torch.mps.current_allocated_memory() / (1024 ** 3):.2f} GB\")\n",
    "    torch.save(model.state_dict(), 'model_' + str(epoch + 1).zfill(3) + '.pth') \n",
    "    \n",
    "# 주의 : 여기서는 편의상 모든 데이터를 train에 사용\n",
    "#       ML에서는 일부 데이터를 validation에 사용하는 것이 일반적       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 보충 코드\n",
    "\n",
    "- [validation loss를 같이 그려서 비교하는 사례](https://www.geeksforgeeks.org/training-and-validation-loss-in-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(128, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일로 저장했던 네트워크의 가중치들 읽어들이기\n",
    "model.load_state_dict(torch.load('model_020.pth', # 모델이 가지고 있는 가중치를 pth 파일로 저장해둠\n",
    "                                 map_location=device, weights_only=True))\n",
    "model.eval() # dropout을 사용하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[258, 2923, 616]\n"
     ]
    }
   ],
   "source": [
    "# 훈련이 끝난 모델에 실제로 일을 시키기 위해\n",
    "idx = tokenizer.encode(\"he killed my\") # 토큰 id의 list\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      " killed\n",
      " my\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([258]))\n",
    "print(tokenizer.decode([2923]))\n",
    "print(tokenizer.decode([616]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "tensor([[ 258, 2923,  616]], device='mps:0')\n",
      "torch.Size([1, 3, 50257])\n",
      "tensor([[[ 2.0657, -9.8385, -9.9497,  ..., -9.8053, -3.2459, -9.8018],\n",
      "         [ 0.3613, -7.9650, -7.5232,  ..., -7.8713, -4.5903, -7.9451],\n",
      "         [-0.4191, -3.5928, -3.4669,  ..., -3.6624, -1.0315, -3.3452]]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "idx = torch.tensor(idx).unsqueeze(0).to(device) # pytorch가 읽을 수 있는 tensor로 바꾼다\n",
    "print(idx.shape)\n",
    "print(idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(idx) # tensor로 바꾼 idx를 model에 집어 넣으면 logits가 나옴\n",
    "\n",
    "print(logits.shape) # torch.Size([1, 3, 50257]) | token 3개, 50257 vocabulary size\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.25\t  parent\n",
      "10.11\t  name\n",
      "9.71\t  long\n",
      "9.40\t  own\n",
      "9.36\t  ma\n",
      "9.13\t  mother\n",
      "9.06\t  ter\n",
      "8.90\t  blood\n",
      "8.88\t  boot\n",
      "8.85\t  o\n",
      " parent\n"
     ]
    }
   ],
   "source": [
    "# 'he killed my'라는 토큰 3개 다음에 뭐가 나오는지 | torch.Size([1, 50257])이므로 다음 단어에 대한 50257개의 확률값이 존재\n",
    "logits = logits[:, -1, :]\n",
    "\n",
    "# 가장 확률이 높은 단어 10개 출력\n",
    "top_logits, top_indices = torch.topk(logits, 10) # 가장 높은 확률의 토큰 10개\n",
    "for p, i in zip(top_logits.squeeze(0).tolist(), top_indices.squeeze(0).tolist()):\n",
    "    print(f'{p:.2f}\\t {tokenizer.decode([i])}') # 토큰 decode하여 보여줌\n",
    "    \n",
    "# 가장 확률이 높은 단어 출력\n",
    "idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "flat = idx_next.squeeze(0) # 배치 차원 제거 torch.Size([1])\n",
    "out = tokenizer.decode(flat.tolist()) # 텐서를 리스트로 바꿔서 디코드\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내부적으로 한단어씩 추측하는 것을 반복\n",
    "# ex.\n",
    "# input >> 'Harry potter is'\n",
    "# output >> a\n",
    "# then update >> 'Harry potter is a' >> use as input\n",
    "# output >> wizard\n",
    "# then update >> 'Harry poter is a wizard' >> 반복\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    for _ in range(max_new_tokens): # 토큰을 몇개까지 반복하여 출력할 것인지\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None: # 확률이 높은 vocabulary 몇개를 골라놓고 그 중에서 선택\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # 확률이 높은 것이 출력될 확률은 높지만, 내부적으로 난수를 발생시켜 출력 | 이 값이 커지면 더 다양하게 답을 할 수 있지만, 헛소리도 증가\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        else: # temperature가 0이면 무조건 확률이 높은 것을 출력\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### about temperature\n",
    "\n",
    "<img src=\"./figure/temperature.webp\" width=\"80%\" height=\"60%\"></img><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Voldemort killed my mother and father, oh no, it wa \n",
      "1 : Voldemort killed my mother becau e  he tried to  top\n",
      "2 : Voldemort killed my mother and father, oh no, it wa \n",
      "3 : Voldemort killed my  he lowered him elf on to hi  front\n",
      "4 : Voldemort killed my mother and father, oh no, it wa \n",
      "5 : Voldemort killed my mother becau e  he tried to rob it\n",
      "6 : Voldemort killed my mother and father, oh no, it wa \n",
      "7 : Voldemort killed my left — Bill wa  head boy and Charlie wa\n",
      "8 : Voldemort killed my mother and father, oh no, it wa \n",
      "9 : Voldemort killed my mother becau e  he tried to  top\n"
     ]
    }
   ],
   "source": [
    "start_context = input(\"Start context : \") # Voldemort killed my라고 입력\n",
    "\n",
    "# idx = tokenizer.encode(start_context, allowed_special={'<|endoftext|>'})\n",
    "idx = tokenizer.encode(start_context)\n",
    "idx = torch.tensor(idx).unsqueeze(0)\n",
    "\n",
    "context_size = model.pos_emb.weight.shape[0] \n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    token_ids = generate( # 위에서 정의한 generate 함수를 이용해 입력된 text의 다음을 10번씩 추측\n",
    "        model=model,\n",
    "        idx=idx.to(device),\n",
    "        max_new_tokens=10,\n",
    "        context_size= context_size,\n",
    "        top_k=10,\n",
    "        temperature=1\n",
    "    )\n",
    "\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    out = tokenizer.decode(flat.tolist()).replace(\"\\n\", \" \")\n",
    "\n",
    "    print(i, \":\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
