{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM configuration practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM configuratino basic process\n",
    "\n",
    "1. pretraining(사전 훈련)으로 일반적인 언어 능력을 가르친다\n",
    "\n",
    "2. fine tuning(미세조정) 단계에서 특정 용도에 맞게 다듬는다   \n",
    "\n",
    "    * 여기까지가 기본적인 절차\n",
    "\n",
    "3. 데이터베이스(혹은 인터넷) 검색 기능을 추가한다   \n",
    "\n",
    "    * 지식의 범위와 정확성을 더 높일 수 있다\n",
    "    \n",
    "4. 내부적으로 질의를 반복하여 더 좋은 결론을 도출한다   \n",
    "\n",
    "    * 사람이 생각을 거듭하여 더 깊이 있는 결론을 이끌어내는 과정과 같다\n",
    "\n",
    "#### the stage of practice   \n",
    "\n",
    "1. 훈련 데이터 준비\n",
    "\n",
    "2. 데이터 로더 정의\n",
    "\n",
    "3. 모델 정의\n",
    "\n",
    "4. 훈련\n",
    "\n",
    "5. 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computer\n",
    "\n",
    "* macbook pro m4 pro\n",
    "\n",
    "#### packages\n",
    "\n",
    "* miniconda=24.11.1\n",
    "\n",
    "* python=3.9.20\n",
    "    ```sh\n",
    "    conda create -n LLM python=3.9.20\n",
    "    ```\n",
    "\n",
    "* tiktoken=0.9.0\n",
    "    ```sh\n",
    "    pip install tiktoken\n",
    "    ```\n",
    "\n",
    "* torch=2.6.0\n",
    "    ```sh\n",
    "    pip3 install torch torchvision torchaudio\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 훈련 데이터 준비\n",
    "\n",
    "* [해리포터 영어 원서](https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books?select=02+Harry+Potter+and+the+Chamber+of+Secrets.txt)\n",
    "    ```sh\n",
    "    curl -o LLM_configuration/data/file-name.txt \"download link\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(filename, path):\n",
    "    with open(path + '/' + filename,  'r', encoding='utf-8') as file:\n",
    "        book_text = file.read()\n",
    "        \n",
    "    cleaned_text = re.sub(r'\\n+', ' ', book_text) # 줄 바꿈을 공백으로\n",
    "    cleaned_text = re.sub(r's+', ' ', cleaned_text) # 연속되는 공백을 하나의 공백으로\n",
    "    \n",
    "    print('cleaned_' + filename, len(cleaned_text), 'characters') # 글자 수 출력\n",
    "    \n",
    "    with open(path + '/' + 'cleaned_' + filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text) # 잘 정리된 훈련 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_01_Harry_Potter_and_the_Sorcerers_Stone.txt 435335 characters\n",
      "cleaned_02_Harry_Potter_and_the_Chamber_of_Secrets.txt 487913 characters\n",
      "cleaned_03_Harry_Potter_and_the_Prisoner_of_Azkaban.txt 619717 characters\n",
      "cleaned_04_Harry_Potter_and_the_Goblet_of_Fire.txt 1091860 characters\n",
      "cleaned_05_Harry_Potter_and_the_Order_of_the_Phoenix.txt 1486941 characters\n",
      "cleaned_06_Harry_Potter_and_the_Half-Blood_Prince.txt 980175 characters\n",
      "cleaned_07_Harry_Potter_and_the_Deathly_Hallows.txt 1131296 characters\n"
     ]
    }
   ],
   "source": [
    "filename_list = ['01_Harry_Potter_and_the_Sorcerers_Stone.txt',\n",
    "                 '02_Harry_Potter_and_the_Chamber_of_Secrets.txt',\n",
    "                 '03_Harry_Potter_and_the_Prisoner_of_Azkaban.txt',\n",
    "                 '04_Harry_Potter_and_the_Goblet_of_Fire.txt',\n",
    "                 '05_Harry_Potter_and_the_Order_of_the_Phoenix.txt',\n",
    "                 '06_Harry_Potter_and_the_Half-Blood_Prince.txt',\n",
    "                 '07_Harry_Potter_and_the_Deathly_Hallows.txt']\n",
    "file_path = '/Users/kh22cho/LLM_configuration/data'\n",
    "\n",
    "for filename in filename_list:\n",
    "    clean_text(filename, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화 진행\n",
    "\n",
    "UTF-8 BPE (Bype Pair Encoding)\n",
    "\n",
    "문자열 데이터를 숫자로 변환하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2') # gpt2에서 사용하는 tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰화의 간단한 예시\n",
    "\n",
    "```python\n",
    "text = 'Harry Potter was a wizard.' # token으로 바꿀 예시 문장 | neural net이 이해할 수 있게 적당히 자른 것이 token\n",
    "\n",
    "tokens = tokenizer.encode(text) # text를 숫자로 바꾸는 과정 \n",
    "\n",
    "print('글자수 :', len(text), '토큰수 :', len(tokens))\n",
    "print(tokens) # 6개의 token들로 text를 변환함 | .도 문장의 끝이라는 의미의 token\n",
    "print(tokenizer.decode(tokens)) # 6개의 token들을 다시 decode하여 text로 원상 복구\n",
    "for t in tokens:\n",
    "    print(f'{t} >>> {tokenizer.decode([t])}')\n",
    "```\n",
    "```\n",
    "글자수 : 26 토큰수 : 6   \n",
    "[18308, 14179, 373, 257, 18731, 13]   \n",
    "Harry Potter was a wizard.   \n",
    "18308 >>> Harry   \n",
    "14179 >>>  Potter   \n",
    "373 >>>  was   \n",
    "257 >>>  a   \n",
    "18731 >>>  wizard   \n",
    "13 >>> .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한글과 한자가 섞여있는 문장에 대한 tokenizer 예제\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer # pip install transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\")  # KoGPT2 사용 | 가장 잘되었던 모델\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")  # KoGPT2 사용\n",
    "\n",
    "print(\"Vocab size :\", len(tokenizer))\n",
    "\n",
    "text = \"대사께서는 도(道)를 얻은 모양이구려.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(len(text), len(tokens))\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 글자 단위로 토큰화하는 예시 | 비효율적이므로 잘 사용하지 않음\n",
    "\n",
    "```python\n",
    "for char in text:\n",
    "    token_ids = tokenizer.encode(char) # 한 글자씩 인코딩(토큰화)\n",
    "    decoded = tokenizer.decode(token_ids) # 한 글자씩 디코딩\n",
    "    print(f\"{char} >>> {token_ids} >>> {decoded}\") # 한 글자 단위이므로 공백도 토큰이 됨\n",
    "```\n",
    "```\n",
    "H >>> [39] >>> H\n",
    "a >>> [64] >>> a\n",
    "r >>> [81] >>> r\n",
    "r >>> [81] >>> r\n",
    "y >>> [88] >>> y\n",
    "  >>> [220] >>>  \n",
    "P >>> [47] >>> P\n",
    "o >>> [78] >>> o\n",
    "t >>> [83] >>> t\n",
    "t >>> [83] >>> t\n",
    "e >>> [68] >>> e\n",
    "r >>> [81] >>> r\n",
    "  >>> [220] >>>  \n",
    "w >>> [86] >>> w\n",
    "a >>> [64] >>> a\n",
    "s >>> [82] >>> s\n",
    "  >>> [220] >>>  \n",
    "a >>> [64] >>> a\n",
    "  >>> [220] >>>  \n",
    "w >>> [86] >>> w\n",
    "i >>> [72] >>> i\n",
    "z >>> [89] >>> z\n",
    "a >>> [64] >>> a\n",
    "r >>> [81] >>> r\n",
    "d >>> [67] >>> d\n",
    ". >>> [13] >>> .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로더 (DataLoader)\n",
    "\n",
    "입력 데이터가 많아질수록 neural net도 무시무시하게 커지기 때문에\n",
    "\n",
    "전체 데이터를 한번에 사용하지 않고, 몇개씩 쪼개주는 기능을 하는 것이 ```Dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt, max_length = 32, stride = 4):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # token_ids = tokenizer.encode(\"<|endoftext|>\" + txt, allowed_special={\"<|endoftext|>\"})\n",
    "        token_ids = tokenizer.encode(txt) # 책 한권을 token으로 바꿈\n",
    "        \n",
    "        print('# of tokens in txt :', len(token_ids)) # token이 몇개정도 되는지\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1] # 앞의 몇개의 단어(input)가 오면, 그 다음으로 오는 단어(target)를 의미\n",
    "            self.input_ids.append(torch.tensor(input_chunk)) # input를 넣으면, \n",
    "            self.target_ids.append(torch.tensor(target_chunk)) # target하고 가급적 똑같은 대답을 하도록 훈련을 시킴\n",
    "            # example\n",
    "            # Text :   Harry Potter was a Wizard.\n",
    "            # input :  Harry Poter\n",
    "            # target :              was\n",
    "            \n",
    "    def __len__(self): # 훈련을 시킬 수 있는 조각이 전부 몇개인지 알려주는 함수\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx): # DataLoader를 사용할때 실제로 호출하는 함수\n",
    "        return self.input_ids[idx], self.target_ids[idx] # input의 id와 target의 id를 쌍으로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open 01_Harry_Potter_and_the_Sorcerers_Stone.txt\n",
      "open 02_Harry_Potter_and_the_Chamber_of_Secrets.txt\n",
      "open 03_Harry_Potter_and_the_Prisoner_of_Azkaban.txt\n",
      "open 04_Harry_Potter_and_the_Goblet_of_Fire.txt\n",
      "open 05_Harry_Potter_and_the_Order_of_the_Phoenix.txt\n",
      "open 06_Harry_Potter_and_the_Half-Blood_Prince.txt\n",
      "open 07_Harry_Potter_and_the_Deathly_Hallows.txt\n",
      "# of tokens in txt : 1912071\n"
     ]
    }
   ],
   "source": [
    "# with open('cleaned_한글문서.txt', 'r', encoding='utf-8-sig') as file: # 선택 : -sig를 붙여서 BOM 제거\n",
    "txt = \"\" # 여러 책의 내용을 합칠 문자열\n",
    "for f in filename_list:\n",
    "    with open(file_path + '/' + 'cleaned_' + f, 'r', encoding='utf-8-sig') as file: # 선택 : -sig를 붙여서 BOM 제거\n",
    "        print('open', f)\n",
    "        txt += file.read() # 책을 읽어온다\n",
    "\n",
    "dataset = MyDataset(txt, max_length = 32, stride = 4) # dataset을 정의\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True, drop_last=True) # dataset을 이용하여 pytorch의 DataLoader를 만든다\n",
    "\n",
    "# 주의 : 여기서는 코드를 단순화하기 위해 test, valid는 생략하고 train_loader만 만들었습니다\n",
    "#       관련된 ML 이론은 train vs test vs validation 등으로 검색하여 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader가 내부적으로 어떻게 작동하는지\n",
    "```python\n",
    "dataiter = iter(train_loader) # loader를 iterator로 바꿔주고\n",
    "\n",
    "x, y = next(dataiter) # next를 이용하여 실제 x, y로 받아온다 | x, y는 각각 input, target이며 token이다\n",
    "\n",
    "print(tokenizer.decode(x[0].tolist()))\n",
    "print(tokenizer.decode(y[0].tolist())) # target이 하나 밀린 형태\n",
    "```\n",
    "```\n",
    "an fer to Slytherin at once, becau e it wa  hi  de tiny. Harry told the turban he didn’t want to\n",
    " fer to Slytherin at once, becau e it wa  hi  de tiny. Harry told the turban he didn’t want to be\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print(x[0])\n",
    "print(y[0]) # target이 하나 밀린 형태 | token으로 보면 더 명확함\n",
    "```\n",
    "```\n",
    "tensor([  272, 11354,   284, 31615,   490,   259,   379,  1752,    11,   639,\n",
    "          559,   304,   340,  2082,   220, 23105,   220,   390,  7009,    13,\n",
    "         5850,  1297,   262,  7858,  3820,   339,  1422,   447,   247,    83,\n",
    "          765,   284])\n",
    "tensor([11354,   284, 31615,   490,   259,   379,  1752,    11,   639,   559,\n",
    "          304,   340,  2082,   220, 23105,   220,   390,  7009,    13,  5850,\n",
    "         1297,   262,  7858,  3820,   339,  1422,   447,   247,    83,   765,\n",
    "          284,   307])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 정의\n",
    "\n",
    "뉴럴 네트워크 모델 정의\n",
    "\n",
    "모델은 교재([Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch))에서 제공하는 예제 코드를 약간 수정한 정도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's parameter\n",
    "\n",
    "# 50257 Tiktoken | 사전에 단어가 몇개나 있는지\n",
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "# VOCAB_SIZE = len(tokenizer) # AutoTokenizer일 경우\n",
    "\n",
    "# \n",
    "CONTEXT_LENGTH = 128 # Shortened context length (orig : 1024)\n",
    "EMB_DIM = 768 # Embedding dimention\n",
    "NUM_HEADS = 12 # Number of attention heads\n",
    "NUM_LAYERS = 12 # Number of layers\n",
    "DROP_RATE = 0.1 # Dropout rate\n",
    "QKV_BIAS = False # Query-key-value bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module): # self attention의 핵심이 되는 부분\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_out % NUM_HEADS == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // NUM_HEADS\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=QKV_BIAS)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(DROP_RATE)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        values = values.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, NUM_HEADS, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module): # neural net 가중치 값들이 너무 들쑥날쑥하지 않고 적당한 범위 내에 몰려있도록 만들어줌\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module): # Linear, GELU activation 다음에 다시 Linear가 있는 간단한 구조\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(EMB_DIM, 4 * EMB_DIM),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * EMB_DIM, EMB_DIM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module): # MultiHeadAttention, FeedForward, LayerNorm등이 사용됨\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=EMB_DIM,\n",
    "            d_out=EMB_DIM)\n",
    "    \n",
    "        self.ff = FeedForward()\n",
    "        self.norm1 = LayerNorm(EMB_DIM)\n",
    "        self.norm2 = LayerNorm(EMB_DIM)\n",
    "        self.drop_shortcut = nn.Dropout(DROP_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module): # GPT model을 사용\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
    "        self.pos_emb = nn.Embedding(CONTEXT_LENGTH, EMB_DIM)\n",
    "        self.drop_emb = nn.Dropout(DROP_RATE)\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock() for _ in range(NUM_LAYERS)]) # TransformerBlock이 정의된 부분으로 이동하면,\n",
    "\n",
    "        self.final_norm = LayerNorm(EMB_DIM)\n",
    "        self.out_head = nn.Linear(EMB_DIM, VOCAB_SIZE, bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer architecture\n",
    "\n",
    "Transformer 구조 중에서 Masked Multi-Head Attention이라는 것은 훈련시키는 기본 원리와 밀접한 관련이 있음\n",
    "\n",
    "오른쪽 그림의 행렬을 보면, 단어끼리 얼마나 관계가 깊은지에 대한 값을 저장한 행렬\n",
    "\n",
    "이번에는 오른쪽 그림의 오른쪽 윗부분을 보면, 숨겨져있는 부분이 있는데 이것이 Masked Multi-Head Attention\n",
    "\n",
    "데이터 셋을 만들때 다음 단어를 추측하는 것과 연관이 되어있음\n",
    "\n",
    "앞부분과 뒷부분을 같이 주면, 다음단어라는 정답을 주는 꼴이 되어버리므로 숨겨서 학습\n",
    "\n",
    "즉, 미래에 오는 단어를 모르게 학습을 하도록 네트워크 구조를 만든 것\n",
    "\n",
    "<img src=\"figure/Transformer_architecture.png\" width=\"30%\">\n",
    "<img src=\"figure/Masked_Multi_Head_Attention.webp\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GELU activation function\n",
    "\n",
    "<img src=\"figure/GELU.png\" width=\"40%\" height=\"30%\"></img><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 훈련\n",
    "\n",
    "nvidia GPU가 아닌 silicon mac의 GPU를 사용하려면 조금 다르게 셋팅이 필요\n",
    "\n",
    "apple에서는 GPU 대신 MPS(Metal Performance Shaders)라고 함\n",
    "\n",
    "MPS는 기존의 CUDA 또는 OpenCL과 같은 라이브러리와 달리 애플에서 직접 제공하는 프레임 워크, 따라서 애플 GPU 기술에 최적화되어 있음\n",
    "\n",
    "- [How to use GPU Acceleration in silicon mac (MPS setting)](https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available in current env : True\n",
      "PyTorch is built with MPS activation : True\n",
      "Current device : mps\n"
     ]
    }
   ],
   "source": [
    "# PyTorch에서 MPS가 활성화되어 있는지 확인하는 방법\n",
    "import torch\n",
    "\n",
    "# 현재 환경에서 MPS를 사용할 수 있는지 여부\n",
    "print('GPU available in current env :', torch.backends.mps.is_available())\n",
    "\n",
    "# 현재 PyTorch 설치가 MPS 활성화로 빌드되었는지 여부 \n",
    "# True : 현재 설치된 PyTorch가 MPS 활성화로 빌드 되었음\n",
    "print('PyTorch is built with MPS activation :', torch.backends.mps.is_built())\n",
    "\n",
    "# MPS를 사용하기 위한 디바이스를 설정\n",
    "device = torch.device('mps')\n",
    "print(\"Current device :\", device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = GPTModel()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
